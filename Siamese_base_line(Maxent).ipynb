{
  "cells": [
    {
      "metadata": {
        "id": "VmL_Z-W4wEKi",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "!pip install seqeval==0.0.5\n",
        "!pip install keras==2.2.4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jFjaBfTzpc-z",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "!pip install git+https://www.github.com/keras-team/keras-contrib.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "AWtgf5isuiv4"
      },
      "cell_type": "code",
      "source": [
        "!pip install pythainlp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "31OaMm4Ym7f2",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "from collections import Counter\n",
        "from tqdm import tqdm\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import string\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers , regularizers\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense , Bidirectional , LSTM, GlobalMaxPool1D, Input, Embedding, MaxPooling1D\n",
        "from tensorflow.keras.layers import Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential, load_model, Model\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras.layers import Embedding\n",
        "from tensorflow. keras.layers import Flatten, Dropout, Activation, Input, Dense, concatenate, GRU, Dropout, Dense, Activation, Flatten, Conv1D, SpatialDropout1D\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.optimizers import Nadam\n",
        "import tensorflow.keras.backend as K\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from pythainlp.corpus import thai_stopwords"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "soJzZJd8O-84",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "tf.__version__"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "O6v2wa4XrKZE"
      },
      "cell_type": "markdown",
      "source": [
        "# Data Preparation "
      ]
    },
    {
      "metadata": {
        "id": "QrMTSJAulv4Z",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "def create_article_label(df):\n",
        "    article_label_encoder = LabelEncoder()\n",
        "    prediction_encoded = article_label_encoder.fit_transform(df.article)\n",
        "    df.insert(df.shape[1], 'label',prediction_encoded ) #แปลง article ที่เกี่ยวข้องเป็น label แบบเลข\n",
        "    prediction_decoded = article_label_encoder.inverse_transform(prediction_encoded)\n",
        "    map_dict = dict(zip(prediction_encoded,prediction_decoded))\n",
        "    return df, article_label_encoder,map_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "o-IQvTJemSxA",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "torts_df = pd.read_pickle('../input/processed-torts/processed_torts20200123.pkl')\n",
        "df, article_label_encoder,map_dict = create_article_label(torts_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "S1XZAvcHqta-",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "#Create label\n",
        "\n",
        "X_plaintiff = []\n",
        "X_defendant = []\n",
        "X_both = []\n",
        "Y = []\n",
        "Y_set = []\n",
        "cases = []\n",
        "for case_id in tqdm(df.case_id.unique()):\n",
        "    Y = np.zeros(article_label_encoder.classes_.shape[0])  \n",
        "    rows = df[df['case_id'] == case_id]   \n",
        "    token = ''\n",
        "    for i, row in rows.iterrows(): \n",
        "        Y[row.label] = 1 \n",
        "        plaintiff_fact_token = row.plaintiff_fact_token\n",
        "        defendant_fact_token = row.defendant_fact_token   \n",
        "    cases.append(case_id)\n",
        "    X_plaintiff.append(plaintiff_fact_token) \n",
        "    X_defendant.append(defendant_fact_token)\n",
        "    X_both.append([' '.join(plaintiff_fact_token),  ' '.join(defendant_fact_token)]) \n",
        "    Y_set.append(Y)\n",
        "Y_set = np.array(Y_set)\n",
        "X_both = np.array(X_both)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RF-fQPgLrgX3",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "freqs = np.zeros(Y_set.shape[1])\n",
        "for col_idx in np.arange(0, Y_set.shape[1]):\n",
        "    freq = np.sum(Y_set[:, col_idx])\n",
        "    freqs[col_idx] = freq\n",
        "sorted_idx = np.argsort(freqs, axis=0)[::-1]\n",
        "sorted_idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "imz3kH_hMCLo",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "for idx in sorted_idx[1:11]:\n",
        "  print(f\"{idx} : \" + map_dict[idx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "8cwjXjY-Fwl2",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "df.groupby(['article','label']).sum()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "4cYX0l0tuiv8"
      },
      "cell_type": "code",
      "source": [
        "def clean_stop(lst):\n",
        "    clean_list = []\n",
        "    stop_words = list(thai_stopwords())\n",
        "    return [word for word in lst if word not in stop_words]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def clean_text(token_list):\n",
        "    clean_lst = []\n",
        "    for token in token_list:\n",
        "        clean_tok = re.sub(r'[\\d๐-๙]','',token)\n",
        "        clean_tok = re.sub(r'[{}]'.format(string.punctuation),'',clean_tok)\n",
        "        if len(clean_tok.strip()) != 0:\n",
        "            clean_lst.append(clean_tok)\n",
        "    return clean_lst"
      ],
      "metadata": {
        "id": "LRtVVR6rxKVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "yFpqxEmZzBzP"
      },
      "cell_type": "markdown",
      "source": [
        "# Evaluation functions"
      ]
    },
    {
      "metadata": {
        "id": "MDqcH1IN5YH4",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "def recall(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "    recall = true_positives / (possible_positives + K.epsilon())\n",
        "    return recall\n",
        "\n",
        "\n",
        "def precision(y_true, y_pred):\n",
        "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "    precision = true_positives / (predicted_positives + K.epsilon())\n",
        "    return precision\n",
        "\n",
        "\n",
        "def microf1(y_true, y_pred):\n",
        "\n",
        "    def recall(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
        "        recall = true_positives / (possible_positives + K.epsilon())\n",
        "        return recall\n",
        "\n",
        "    def precision(y_true, y_pred):\n",
        "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
        "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
        "        precision = true_positives / (predicted_positives + K.epsilon())\n",
        "        return precision\n",
        "\n",
        "    precision = precision(y_true, y_pred)\n",
        "    recall = recall(y_true, y_pred)\n",
        "    return 2 * ((precision * recall) / (precision + recall + K.epsilon()))\n",
        "\n",
        "\n",
        "def macrof1(y_true, y_pred):\n",
        "    y_pred = K.round(y_pred)\n",
        "    tp = K.sum(K.cast(y_true * y_pred, 'float'), axis=0)\n",
        "    # tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
        "    fp = K.sum(K.cast((1 - y_true) * y_pred, 'float'), axis=0)\n",
        "    fn = K.sum(K.cast(y_true * (1 - y_pred), 'float'), axis=0)\n",
        "\n",
        "    p = tp / (tp + fp + K.epsilon())\n",
        "    r = tp / (tp + fn + K.epsilon())\n",
        "\n",
        "    f1 = 2 * p * r / (p + r + K.epsilon())\n",
        "    f1 = tf.where(tf.math.is_nan(f1), tf.zeros_like(f1), f1)\n",
        "    return K.mean(f1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Only plaintiff as feature** "
      ],
      "metadata": {
        "id": "O04QzKZixvd8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature engineering functions"
      ],
      "metadata": {
        "id": "1V7EvwbjxZWD"
      }
    },
    {
      "metadata": {
        "trusted": true,
        "id": "EmH766VYuiv_"
      },
      "cell_type": "code",
      "source": [
        "def featurize(token_list):\n",
        "    token_list = clean_text(token_list)\n",
        "    dic = {}\n",
        "    for i in range(len(token_list)):\n",
        "        dic[token_list[i]] = 1\n",
        "    return dic\n",
        "\n",
        "def unibi_featurize(token_list):\n",
        "  token_list = clean_text(token_list)\n",
        "  unibi_dic = {}\n",
        "  for i in range(len(token_list)-1):\n",
        "    unibi_dic[token_list[i]] = 1\n",
        "    unibi_dic[token_list[i] + '_' +token_list[i+1]] = 1\n",
        "  return unibi_dic"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "aphNI01Wuiv_"
      },
      "cell_type": "code",
      "source": [
        "unibi_featurize(X_plaintiff[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "kRWZG0JIuiwA"
      },
      "cell_type": "code",
      "source": [
        "featurize(X_plaintiff[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "bMyDzSh2r8SA"
      },
      "cell_type": "markdown",
      "source": [
        "Training Maxent \n",
        "===============\n"
      ]
    },
    {
      "metadata": {
        "trusted": true,
        "id": "7tv7IduMuiwA"
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer, TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.feature_extraction import DictVectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "b8mqDvOwuiwA"
      },
      "cell_type": "code",
      "source": [
        "#Tfidf\n",
        "\n",
        "for f_num in sorted_idx[1:11]:   \n",
        "    vectorizer = TfidfVectorizer(ngram_range=(1, 1))\n",
        "    X = X_both[:, 0]\n",
        "    Y = Y_set[:, f_num].astype(int)\n",
        "  \n",
        "    X_train, X_rest, Y_train, Y_rest = train_test_split(X, Y, test_size=0.3, random_state=42)    #train_seqs-Y_train   , test_seqs - Y_test\n",
        "    X_test, X_dev, Y_test, Y_dev = train_test_split(X_rest, Y_rest, test_size=0.5, random_state=42) \n",
        "    X_train = vectorizer.fit_transform(X_train)\n",
        "    model = LogisticRegression(solver='liblinear', n_jobs=1, verbose=1, random_state=42, C=1e5, penalty='l2', max_iter=2000)\n",
        "    model.fit(X_train, Y_train)\n",
        "    \n",
        "    X_test = vectorizer.transform(X_test)\n",
        "    \n",
        "    print(\"Evaluation\")\n",
        "    print(f'<<<<<<label {f_num}>>>>>>>>' )\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    \n",
        "    print('f1 : ')\n",
        "    print(f1_score(Y_test, y_pred,labels=[1]))\n",
        "    print('------------------------------------------------------------ ')\n",
        "    print('precision : ')\n",
        "    print(precision_score(Y_test, y_pred,labels=[1]))\n",
        "    print('------------------------------------------------------------ ')\n",
        "    print('recall : ')\n",
        "    print(recall_score(Y_test, y_pred,labels=[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "kpq3JfyJuiwB"
      },
      "cell_type": "code",
      "source": [
        "#bag of words\n",
        "\n",
        "X_bow_uni = [featurize(lst) for lst in X_plaintiff]\n",
        "\n",
        "for f_num in sorted_idx[1:11]:   \n",
        "    vectorizer = DictVectorizer(sparse=True)\n",
        "    X = X_bow_uni\n",
        "    Y = Y_set[:, f_num].astype(int)\n",
        "  \n",
        "    X_train, X_rest, Y_train, Y_rest = train_test_split(X, Y, test_size=0.3, random_state=42)    #train_seqs-Y_train   , test_seqs - Y_test\n",
        "    X_test, X_dev, Y_test, Y_dev = train_test_split(X_rest, Y_rest, test_size=0.5, random_state=42) \n",
        "    X_train = vectorizer.fit_transform(X_train)\n",
        "    model = LogisticRegression(solver='liblinear', n_jobs=1, verbose=1, random_state=42, C=1e5, penalty='l2', max_iter=2000)\n",
        "    model.fit(X_train, Y_train)\n",
        "    \n",
        "    X_test = vectorizer.transform(X_test)\n",
        "    \n",
        "    print(\"Evaluation\")\n",
        "    print(f'<<<<<<label {f_num}>>>>>>>>' )\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    \n",
        "    print('f1 : ')\n",
        "    print(f1_score(Y_test, y_pred,labels=[1]))\n",
        "    print('------------------------------------------------------------ ')\n",
        "    print('precision : ')\n",
        "    print(precision_score(Y_test, y_pred,labels=[1]))\n",
        "    print('------------------------------------------------------------ ')\n",
        "    print('recall : ')\n",
        "    print(recall_score(Y_test, y_pred,labels=[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "I4ewR8eouiwB"
      },
      "cell_type": "code",
      "source": [
        "#unigram + bigram\n",
        "\n",
        "X_bow = [unibi_featurize(lst) for lst in X_plaintiff]\n",
        "\n",
        "for f_num in sorted_idx[1:11]:   \n",
        "    vectorizer = DictVectorizer(sparse=True)\n",
        "    X = X_bow\n",
        "    Y = Y_set[:, f_num].astype(int)\n",
        "  \n",
        "    X_train, X_rest, Y_train, Y_rest = train_test_split(X, Y, test_size=0.3, random_state=42)    #train_seqs-Y_train   , test_seqs - Y_test\n",
        "    X_test, X_dev, Y_test, Y_dev = train_test_split(X_rest, Y_rest, test_size=0.5, random_state=42) \n",
        "    X_train = vectorizer.fit_transform(X_train)\n",
        "    model = LogisticRegression(solver='liblinear', n_jobs=1, verbose=1, random_state=42, C=1e5, penalty='l2', max_iter=2000)\n",
        "    model.fit(X_train, Y_train)\n",
        "    \n",
        "    X_test = vectorizer.transform(X_test)\n",
        "    \n",
        "    print(\"Evaluation\")\n",
        "    print(f'<<<<<<label {f_num}>>>>>>>>' )\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    \n",
        "    print('f1 : ')\n",
        "    print(f1_score(Y_test, y_pred,labels=[1]))\n",
        "    print('------------------------------------------------------------ ')\n",
        "    print('precision : ')\n",
        "    print(precision_score(Y_test, y_pred,labels=[1]))\n",
        "    print('------------------------------------------------------------ ')\n",
        "    print('recall : ')\n",
        "    print(recall_score(Y_test, y_pred,labels=[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Both plaintiff and defendant as features**"
      ],
      "metadata": {
        "id": "DNYRuFF0v7PE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Feature engineering functions"
      ],
      "metadata": {
        "id": "UFqHlWmdyGvD"
      }
    },
    {
      "metadata": {
        "trusted": true,
        "id": "sewrLohLuiwB"
      },
      "cell_type": "code",
      "source": [
        "#unigram\n",
        "def two_facts_feature(X1,X2):\n",
        "    X_facts_feature = {}\n",
        "    X1 = clean_text(X1)\n",
        "    X2 = clean_text(X2)\n",
        "    for p in X1:\n",
        "        X_facts_feature[p] = 1\n",
        "    for d in X2:\n",
        "        X_facts_feature[d] = 1\n",
        "    return X_facts_feature"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "kCNYoz18uiwC"
      },
      "cell_type": "code",
      "source": [
        " #identify source\n",
        "def two_facts_uni_feature(X1,X2): \n",
        "    X_facts_feature = {}\n",
        "    X1 = clean_text(X1)\n",
        "    X2 = clean_text(X2)\n",
        "    for p in X1:\n",
        "        X_facts_feature[p + '_' + 'plaintiff'] = 1\n",
        "        X_facts_feature[p] = 1\n",
        "    for d in X2:\n",
        "        X_facts_feature[d + '_' + 'defendant'] = 1\n",
        "        X_facts_feature[d] = 1\n",
        "    return X_facts_feature"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "h9z_XarMuiwD"
      },
      "cell_type": "code",
      "source": [
        "def two_facts_unibi_feature(X1,X2):\n",
        "    X_facts_feature = {}\n",
        "    X1 = clean_text(X1)\n",
        "    X2 = clean_text(X2)\n",
        "    for p in X1:\n",
        "        X_facts_feature[p + '_' + 'plaintiff'] = 1\n",
        "        X_facts_feature[p] = 1\n",
        "    for i in range(len(X1)-1):\n",
        "        X_facts_feature[X1[i] + '_'+ X1[i+1] + '_' + 'plaintiff'] = 1\n",
        "        X_facts_feature[X1[i] + '_'+ X1[i+1]] = 1\n",
        "    for i in range(len(X2)-1):\n",
        "        X_facts_feature[X2[i] + '_'+ X2[i+1] + '_' + 'defendant'] = 1\n",
        "        X_facts_feature[X2[i] + '_'+ X2[i+1]] = 1\n",
        "    return X_facts_feature"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "LkgarznLuiwC"
      },
      "cell_type": "code",
      "source": [
        "two_facts_feature(X_plaintiff[0],X_defendant[0])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "Gzd9M45LuiwC"
      },
      "cell_type": "code",
      "source": [
        "two_facts_uni_feature(X_plaintiff[1],X_defendant[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "AxUE7aeWuiwD"
      },
      "cell_type": "code",
      "source": [
        "two_facts_unibi_feature(X_plaintiff[1],X_defendant[1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training Maxent "
      ],
      "metadata": {
        "id": "rB7A8T69yUIT"
      }
    },
    {
      "metadata": {
        "trusted": true,
        "id": "jfpDLaXKuiwC"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "X_two_feature = [two_facts_feature(lst1,lst2) for lst1,lst2 in zip(X_plaintiff,X_defendant)]\n",
        "for f_num in sorted_idx[1:11]:   \n",
        "    vectorizer = DictVectorizer(sparse=True)\n",
        "    X = X_two_feature\n",
        "    Y = Y_set[:, f_num].astype(int)\n",
        "  \n",
        "    X_train, X_rest, Y_train, Y_rest = train_test_split(X, Y, test_size=0.3, random_state=42)    #train_seqs-Y_train   , test_seqs - Y_test\n",
        "    X_test, X_dev, Y_test, Y_dev = train_test_split(X_rest, Y_rest, test_size=0.5, random_state=42) \n",
        "    X_train = vectorizer.fit_transform(X_train)\n",
        "    model = LogisticRegression(solver='liblinear', n_jobs=1, verbose=1, random_state=42, C=1e5, penalty='l2', max_iter=2000)\n",
        "    model.fit(X_train, Y_train)\n",
        "    \n",
        "    X_test = vectorizer.transform(X_test)\n",
        "    \n",
        "    print(\"Evaluation\")\n",
        "    print(f'<<<<<<label {f_num}>>>>>>>>' )\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    \n",
        "    print('f1 : ')\n",
        "    print(f1_score(Y_test, y_pred,labels=[1]))\n",
        "    print('------------------------------------------------------------ ')\n",
        "    print('precision : ')\n",
        "    print(precision_score(Y_test, y_pred,labels=[1]))\n",
        "    print('------------------------------------------------------------ ')\n",
        "    print('recall : ')\n",
        "    print(recall_score(Y_test, y_pred,labels=[1]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "MnEGeJp6uiwC"
      },
      "cell_type": "code",
      "source": [
        "X_two_uni = [two_facts_uni_feature(lst1,lst2) for lst1,lst2 in zip(X_plaintiff,X_defendant)]\n",
        "\n",
        "for f_num in sorted_idx[1:11]:   \n",
        "    vectorizer = DictVectorizer(sparse=True)\n",
        "    X = X_two_uni\n",
        "    Y = Y_set[:, f_num].astype(int)\n",
        "  \n",
        "    X_train, X_rest, Y_train, Y_rest = train_test_split(X, Y, test_size=0.3, random_state=42)    #train_seqs-Y_train   , test_seqs - Y_test\n",
        "    X_test, X_dev, Y_test, Y_dev = train_test_split(X_rest, Y_rest, test_size=0.5, random_state=42) \n",
        "    X_train = vectorizer.fit_transform(X_train)\n",
        "    model = LogisticRegression(solver='liblinear', n_jobs=1, verbose=1, random_state=42, C=1e5, penalty='l2', max_iter=2000)\n",
        "    model.fit(X_train, Y_train)\n",
        "    \n",
        "    X_test = vectorizer.transform(X_test)\n",
        "    \n",
        "    print(\"Evaluation\")\n",
        "    print(f'<<<<<<label {f_num}>>>>>>>>' )\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    \n",
        "    print('f1 : ')\n",
        "    print(f1_score(Y_test, y_pred,labels=[1]))\n",
        "    print('------------------------------------------------------------ ')\n",
        "    print('precision : ')\n",
        "    print(precision_score(Y_test, y_pred,labels=[1]))\n",
        "    print('------------------------------------------------------------ ')\n",
        "    print('recall : ')\n",
        "    print(recall_score(Y_test, y_pred,labels=[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "id": "ZTIw8lW6uiwD"
      },
      "cell_type": "code",
      "source": [
        "X_two_uni_bi = [two_facts_unibi_feature(lst1,lst2) for lst1,lst2 in zip(X_plaintiff,X_defendant)]\n",
        "\n",
        "for f_num in sorted_idx[1:11]:   \n",
        "    vectorizer = DictVectorizer(sparse=True)\n",
        "    X = X_two_uni_bi\n",
        "    Y = Y_set[:, f_num].astype(int)\n",
        "  \n",
        "    X_train, X_rest, Y_train, Y_rest = train_test_split(X, Y, test_size=0.3, random_state=42)    #train_seqs-Y_train   , test_seqs - Y_test\n",
        "    X_test, X_dev, Y_test, Y_dev = train_test_split(X_rest, Y_rest, test_size=0.5, random_state=42) \n",
        "    X_train = vectorizer.fit_transform(X_train)\n",
        "    model = LogisticRegression(solver='liblinear', n_jobs=1, verbose=1, random_state=42, C=1e5, penalty='l2', max_iter=2000)\n",
        "    model.fit(X_train, Y_train)\n",
        "    \n",
        "    X_test = vectorizer.transform(X_test)\n",
        "    \n",
        "    print(\"Evaluation\")\n",
        "    print(f'<<<<<<label {f_num}>>>>>>>>' )\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    \n",
        "    print('f1 : ')\n",
        "    print(f1_score(Y_test, y_pred,labels=[1]))\n",
        "    print('------------------------------------------------------------ ')\n",
        "    print('precision : ')\n",
        "    print(precision_score(Y_test, y_pred,labels=[1]))\n",
        "    print('------------------------------------------------------------ ')\n",
        "    print('recall : ')\n",
        "    print(recall_score(Y_test, y_pred,labels=[1]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "KM0OvX6HzB1a"
      },
      "cell_type": "markdown",
      "source": [
        "# Evaluation"
      ]
    },
    {
      "metadata": {
        "id": "CmnKROURzB1l",
        "trusted": true
      },
      "cell_type": "code",
      "source": [
        "for f_num in sorted_idx[1:11]:   \n",
        "    vectorizer = TfidfVectorizer(ngram_range=(1, 1))\n",
        "    X = X_both\n",
        "    Y = Y_set[:, f_num].astype(int)\n",
        "  \n",
        "    X_train, X_rest, Y_train, Y_rest = train_test_split(X, Y, test_size=0.3, random_state=42)    \n",
        "    X_test, X_dev, Y_test, Y_dev = train_test_split(X_rest, Y_rest, test_size=0.5, random_state=42) \n",
        "    \n",
        "    X_train_0 = vectorizer.fit_transform(X_train[:, 0])\n",
        "    X_test_0 = vectorizer.transform(X_test[:, 0])\n",
        "    X_train_1 = vectorizer.fit_transform(X_train[:, 1])\n",
        "    X_test_1 = vectorizer.transform(X_test[:, 1])\n",
        "    X_train = np.hstack([X_train_0.todense(), X_train_1.todense()])\n",
        "    model = LogisticRegression(solver='liblinear', n_jobs=1, verbose=1, random_state=42, C=1e5, penalty='l2', max_iter=2000)\n",
        "    model.fit(X_train, Y_train)\n",
        "    \n",
        "    X_2 = np.hstack([X_test_0.todense(), X_test_1.todense()])\n",
        "    \n",
        "    print(\"Evaluation\")\n",
        "    print(f'<<<<<<label {f_num}>>>>>>>>' )\n",
        "\n",
        "    y_pred = model.predict(X_2)\n",
        "    \n",
        "    print('f1 : ')\n",
        "    print(f1_score(Y_test, y_pred,labels=[1]))\n",
        "    print('------------------------------------------------------------ ')\n",
        "    print('precision : ')\n",
        "    print(precision_score(Y_test, y_pred,labels=[1]))\n",
        "    print('------------------------------------------------------------ ')\n",
        "    print('recall : ')\n",
        "    print(recall_score(Y_test, y_pred,labels=[1]))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3",
      "language": "python"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.9",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "Siamese-base-line(Maxent).ipynb",
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}